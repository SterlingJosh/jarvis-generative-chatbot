{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative ChatBot using Amazon SageMaker Seq2Seq built-in algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. **This should be within the same region as the Notebook Instance, training, and hosting.**\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp in the cell below with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# S3 bucket and prefix\n",
    "bucket = 'sagemaker-generative-chatbot-exp'\n",
    "experiment_name = 'GCB-BEST'\n",
    "prefix = experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install contraction framework to expand words like \"i'm\" into \"i am\", \"we're\" into \"we are\", etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the Python libraries we'll need for the remainder of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sentenceTokenizer import SentenceTokeniser\n",
    "import contractions\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# For plotting attention matrix later on\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://sagemaker-generative-chatbot-exp/raw-training-data/cornell-movie-corpus . --recursive     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a conversation pairs from the raw movie dialog corpus.\n",
    "1. We read all the chat lines into a dictionary\n",
    "2. Stitch the chat line into conversation dialogs\n",
    "3. Clean and expand the chat lines\n",
    "4. Limit the Vocab to 15k to remove non popular words which makes it easier for the model to learn. Also remove the training sets with OOV (Out of Vocabulary)\n",
    "5. Generate list of conversation pairs\n",
    "6. Clean and expand the sentences\n",
    "7. Split into train/val files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 20    # 20 words max per sequence\n",
    "MAX_TRAINING_SAMPLES = -1   # Take all the training samples with no OOV\n",
    "MAX_VOCAB = 15000           # Cap vocab to 15k\n",
    "\n",
    "train_val_split = 0.1\n",
    "SPLIT_STRING = \" +++$+++ \"\n",
    "chat_lines_data_path = \"movie_lines.txt\"\n",
    "convo_data_path = \"movie_conversations.txt\"\n",
    "\n",
    "sentence_tokenizer = SentenceTokeniser()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove any html/xml tag\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.replace(\"...\", \" \")\n",
    "    text = text.replace(\"..\", \" \")\n",
    "    text = text.replace(\" - \", '. ')\n",
    "    text = text.replace(\"-\", ' ')\n",
    "    text = text.replace(\"  \", ' ')\n",
    "\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Separate punctuations to reduce unecassery vocabs\n",
    "    text = text.replace(\"?\", \" ?\")\n",
    "    text = text.replace(\"!\", \" !\")\n",
    "    text = text.replace(\".\", \" .\")\n",
    "    text = text.replace(\",\", \" ,\")\n",
    "\n",
    "    text = re.sub(r\"  +\", \" \", text)\n",
    "    text = re.sub(r\"didn'\", \"did not\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    \n",
    "    # Replace tryin' into trying but don't replace man's into mangs\n",
    "    text = re.sub(r\"n'[^s]\", \"ng\", text)\n",
    "\n",
    "    text = re.sub(r\"[^a-z0-9,.?!' ]+\", '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_final_sentences(sentences):\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.rstrip(\".\").rstrip(\",\")\n",
    "        # Limit sentence to 20 words max\n",
    "        sentence = \" \".join(sentence.split(\" \")[:MAX_SENTENCE_LENGTH])\n",
    "        cleaned_sentences.append(sentence)\n",
    "\n",
    "    return cleaned_sentences\n",
    "\n",
    "def extract_ngram_pair(text_lines):\n",
    "    sentences = []\n",
    "    for part in text_lines:\n",
    "        sub_sentences = sentence_tokenizer.tokenizeSentence(clean_text(part))\n",
    "        cleaned_sub_sentences = clean_final_sentences(sub_sentences)\n",
    "        sentences.extend(cleaned_sub_sentences)\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(1, len(sentences), 1):\n",
    "        qst = sentences[i - 1]\n",
    "        ans = sentences[i]\n",
    "\n",
    "        pairs.append((qst, ans))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def collect_vocab(new_pairs, vocabs):\n",
    "    for pair in new_pairs:\n",
    "        for i in range(2):\n",
    "            words = pair[i].split(\" \")\n",
    "            for word in words:\n",
    "                if word in vocabs:\n",
    "                    vocabs[word] += 1\n",
    "                else:\n",
    "                    vocabs[word] = 1\n",
    "\n",
    "def write_to_file(combined_path, source_path, target_path, pairs):\n",
    "    with open(combined_path, \"w\") as f:\n",
    "        for pair in pairs:\n",
    "            f.write(f\"{pair[0]} <--> {pair[1]}\\n\")\n",
    "        f.close()\n",
    "\n",
    "    with open(source_path, \"w\") as f:\n",
    "        for pair in pairs:\n",
    "            f.write(pair[0] + \"\\n\")\n",
    "        f.close()\n",
    "\n",
    "    with open(target_path, \"w\") as f:\n",
    "        for pair in pairs:\n",
    "            f.write(pair[1] + \"\\n\")\n",
    "        f.close()\n",
    "\n",
    "def sentence_in_vocab(sentence, vocabs):\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word not in vocabs:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "chat_lines = {}\n",
    "\n",
    "with open(chat_lines_data_path, \"r\", errors='replace') as f:\n",
    "    text_lines = f.readlines()\n",
    "\n",
    "    for line in text_lines:\n",
    "        tokens = line.split(SPLIT_STRING)\n",
    "        line_id = tokens[0]\n",
    "        line_convo = tokens[-1]\n",
    "        chat_lines[line_id] = line_convo\n",
    "        \n",
    "conversation_list = []\n",
    "\n",
    "with open(convo_data_path, \"r\") as f:\n",
    "    text_lines = f.readlines()\n",
    "    pairs = []\n",
    "\n",
    "    for line in text_lines:\n",
    "        tokens = line.split(SPLIT_STRING)\n",
    "        convo_lines = re.sub(r\"[^L0-9,]+\", \"\", tokens[3]).split(\",\")\n",
    "\n",
    "        cur_chat_lines = []\n",
    "        for convo_line in convo_lines:\n",
    "            cur_chat_lines.append(chat_lines[convo_line])\n",
    "\n",
    "        conversation_list.append(cur_chat_lines)\n",
    "        \n",
    "pairs = []\n",
    "vocabs = {}\n",
    "\n",
    "# Generate a conversation pairs\n",
    "for i, conversation in enumerate(conversation_list):\n",
    "    new_pairs = extract_ngram_pair(conversation)\n",
    "    collect_vocab(new_pairs, vocabs)\n",
    "    pairs.extend(new_pairs)\n",
    "\n",
    "# Get the most used vocabs\n",
    "if MAX_VOCAB != -1:\n",
    "    vocab_list = [(vocab, count) for vocab, count in vocabs.items()]\n",
    "    vocab_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    vocab_list = vocab_list[:MAX_VOCAB]\n",
    "    \n",
    "    vocabs = {vocab[0]: True for vocab in vocab_list}\n",
    "    \n",
    "    # Remove pair which has word not in vocab\n",
    "    used_pairs = []\n",
    "    for pair in pairs:\n",
    "        if sentence_in_vocab(pair[0], vocabs) and sentence_in_vocab(pair[1], vocabs):\n",
    "            used_pairs.append(pair)\n",
    "            \n",
    "    print(f\"Pairs len {len(pairs)} after vocab prunning {len(used_pairs)}\")\n",
    "    pairs = used_pairs\n",
    "    \n",
    "    print(\"Popular vocabs\")\n",
    "    for vocab in vocab_list[:10]:\n",
    "        print(f\"{vocab[0]} : Count={vocab[1]}\\n\")\n",
    "\n",
    "if MAX_TRAINING_SAMPLES != -1:\n",
    "    pairs = pairs[:MAX_TRAINING_SAMPLES]\n",
    "\n",
    "# Dump the first 10\n",
    "for pair in pairs[:10]:\n",
    "    print(f\"Q: {pair[0]}\")\n",
    "    print(f\"A: {pair[1]}\\n\")\n",
    "\n",
    "print(f\"Vocab Len {len(vocabs)} Pairs Len {len(pairs)}\")\n",
    "\n",
    "num_val = int(len(pairs) * train_val_split)\n",
    "\n",
    "train_combined_path = \"train-combined.txt\"\n",
    "val_combined_path = \"val-combined.txt\"\n",
    "\n",
    "train_source_path = \"train-source.txt\"\n",
    "val_source_path = \"val-source.txt\"\n",
    "train_target_path = \"train-target.txt\"\n",
    "val_target_path = \"val-target.txt\"\n",
    "\n",
    "write_to_file(val_combined_path, val_source_path, val_target_path, pairs[-num_val:])\n",
    "write_to_file(train_combined_path, train_source_path, train_target_path, pairs[:-num_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the preprocessing script `create_vocab_proto.py` (provided with this notebook) to create vocabulary mappings (strings to integers) and convert these files to x-recordio-protobuf as required for training by SageMaker Seq2Seq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "python3 create_vocab_proto.py \\\n",
    "        --train-source train-source.txt \\\n",
    "        --train-target train-target.txt \\\n",
    "        --val-source val-source.txt \\\n",
    "        --val-target val-target.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script will output 4 files, namely:\n",
    "- train.rec : Contains source and target sentences for training in protobuf format\n",
    "- val.rec : Contains source and target sentences for validation in protobuf format\n",
    "- vocab.src.json : Vocabulary mapping (string to int) for source sentences\n",
    "- vocab.trg.json : Vocabulary mapping (string to int) for target sentences\n",
    "\n",
    "Let's upload the pre-processed dataset and vocabularies to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(bucket, prefix, channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = prefix + \"/\" + channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "upload_to_s3(bucket, prefix, 'train', 'train.rec')\n",
    "upload_to_s3(bucket, prefix, 'validation', 'val.rec')\n",
    "upload_to_s3(bucket, prefix, 'vocab', 'vocab.src.json')\n",
    "upload_to_s3(bucket, prefix, 'vocab', 'vocab.trg.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(region_name, 'seq2seq')\n",
    "\n",
    "print('Using SageMaker Seq2Seq container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = experiment_name + strftime(\"%Y-%m-%d-%H-%M\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/\".format(bucket, prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        # Seq2Seq does not support multiple machines. Currently, it only supports single machine, multiple GPUs\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.p3.8xlarge\", # We suggest one of [\"ml.p2.16xlarge\", \"ml.p2.8xlarge\", \"ml.p2.xlarge\"]\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        # Please refer to the documentation for complete list of parameters\n",
    "        \"max_seq_len_source\": \"20\",\n",
    "        \"max_seq_len_target\": \"20\",\n",
    "        \"optimized_metric\": \"bleu\",\n",
    "        \"bleu_sample_size\": \"1000\",\n",
    "        \"batch_size\": \"512\",\n",
    "        \"checkpoint_frequency_num_batches\": \"1000\",\n",
    "        \"rnn_num_hidden\": \"2048\",\n",
    "        \"num_layers_encoder\": \"1\",\n",
    "        \"num_layers_decoder\": \"1\",\n",
    "        \"num_embed_source\": \"512\",\n",
    "        \"num_embed_target\": \"512\",\n",
    "        \"max_num_batches\": \"40100\",\n",
    "        \"checkpoint_threshold\": \"3\"\n",
    "        # \"max_num_batches\": \"2100\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 5 * 3600 # Run for maximum 5 hours first\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/train/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"vocab\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/vocab/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/validation/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "sagemaker_client = boto3.Session().client(service_name='sagemaker')\n",
    "sagemaker_client.create_training_job(**create_training_params)\n",
    "\n",
    "status = sagemaker_client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = sagemaker_client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "# if the job failed, determine why\n",
    "if status == 'Failed':\n",
    "    message = sagemaker_client.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now wait for the training job to complete and proceed to the next step after you see model artifacts in your S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can jump to [Use a pretrained model](#Use-a-pretrained-model) as training might take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference.\n",
    "This section involves several steps,\n",
    "- Create model - Create a model using the artifact (model.tar.gz) produced by training\n",
    "- Create Endpoint Configuration - Create a configuration defining an endpoint, using the above model\n",
    "- Create Endpoint - Use the configuration to create an inference endpoint.\n",
    "- Perform Inference - Perform inference on some input data using the endpoint.\n",
    "\n",
    "### Create model\n",
    "We now create a SageMaker Model from the training output. Using the model, we can then create an Endpoint Configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a pretrained model\n",
    "#### Please uncomment and run the cell below if you want to use a pretrained model, as training might take several hours/days to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_pretrained_model = True\n",
    "#model_name = \"your-model-name-here\"\n",
    "#!curl https://s3-us-west-2.amazonaws.com/seq2seq-data/model.tar.gz > model.tar.gz\n",
    "#!curl https://s3-us-west-2.amazonaws.com/seq2seq-data/vocab.src.json > vocab.src.json\n",
    "#!curl https://s3-us-west-2.amazonaws.com/seq2seq-data/vocab.trg.json > vocab.trg.json\n",
    "#upload_to_s3(bucket, prefix, 'pretrained_model', 'model.tar.gz')\n",
    "#model_data = \"s3://{}/{}/pretrained_model/model.tar.gz\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sage = boto3.client('sagemaker')\n",
    "\n",
    "if not use_pretrained_model:\n",
    "    info = sage.describe_training_job(TrainingJobName=job_name)\n",
    "    model_name=job_name\n",
    "    model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "print(model_name)\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = sage.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "Use the model to create an endpoint configuration. The endpoint configuration also contains information about the type and number of EC2 instances to use when hosting the model.\n",
    "\n",
    "Since SageMaker Seq2Seq is based on Neural Nets, we could use an ml.p2.xlarge (GPU) instance, but for this example we will use a free tier eligible ml.m4.xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'GCB-EndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sage.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "Lastly, we create the endpoint that serves up model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 10-15 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = experiment_name + '-Endpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sage.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "# wait until the status has changed\n",
    "sage.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message,\n",
    "> Endpoint creation ended with EndpointStatus = InService\n",
    "\n",
    "then congratulations! You now have a functioning inference endpoint. You can confirm the endpoint configuration and status by navigating to the \"Endpoints\" tab in the AWS SageMaker console.  \n",
    "\n",
    "We will finally create a runtime object from which we can invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.client(service_name='runtime.sagemaker') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using JSON format for inference (Suggested for a single or small number of data instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "sentences.extend([\"how are you?\",\n",
    "             \"hello\",\n",
    "             \"what is your name?\",\n",
    "             \"where do you live?\",\n",
    "             \"i am very sad right now\",\n",
    "             \"what is your favourite song?\",\n",
    "             \"i do not want to talk to you anymore\"\n",
    "            ])\n",
    "\n",
    "sentences.extend([\"can you swim?\",\n",
    "             \"where do you come from?\",\n",
    "             \"do you know albert einstein?\",\n",
    "             \"you suck bad!\",\n",
    "             \"i am going to kill you\",\n",
    "             \"do you want to go out with me?\",\n",
    "             \"let us talk about something fun\"\n",
    "            ])\n",
    "\n",
    "sentences.extend([\"what is the purpose of life?\",\n",
    "             \"what time it is now?\",\n",
    "             \"do you have a hobby?\",\n",
    "             \"who is your best friend?\",\n",
    "             \"i do not know what you are talking about\",\n",
    "             \"are you alive?\",\n",
    "             \"do you join a hackathon team?\"\n",
    "            ])\n",
    "\n",
    "sentences.extend([\"what is your favourite sport?\",\n",
    "             \"you are nasty!\",\n",
    "             \"here you go, you did it again\",\n",
    "             \"do you know who i am?\",\n",
    "             \"do you know what i am capable of?\",\n",
    "             \"whats your sisters number?\",\n",
    "             \"which team are you on?\"\n",
    "            ])\n",
    "\n",
    "sentences.extend([\"huh?\",\n",
    "             \"why is that?\",\n",
    "             \"why did you kill friedman?\",\n",
    "             \"super awesome\",\n",
    "             \"you do not listen to me\",\n",
    "             \"unbelievable!\",\n",
    "             \"where did you go last night?\"\n",
    "            ])\n",
    "\n",
    "sentences.extend([\"how old are you?\",\n",
    "             \"why do we dream?\",\n",
    "             \"can we go out today?\",\n",
    "             \"can you please shut up!\",\n",
    "             \"i am sick of you\",\n",
    "             \"i am super hungry right now\",\n",
    "             \"what did you say again?\"\n",
    "            ])\n",
    "\n",
    "payload = {\"instances\" : []}\n",
    "for sent in sentences:\n",
    "    payload[\"instances\"].append({\"data\" : clean_text(sent)})\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/json', \n",
    "                                   Body=json.dumps(payload))\n",
    "\n",
    "response = response[\"Body\"].read().decode(\"utf-8\")\n",
    "response = json.loads(response)\n",
    "for i, pred in enumerate(response['predictions']):\n",
    "    print(f\"Human: {sentences[i]}\\nJarvis: {pred['target']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop / Close the Endpoint (Optional)\n",
    "\n",
    "Finally, we should delete the endpoint before we close the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
